{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split(x,y,k,m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    # ns is the number of elements in each split\n",
    "    # print (\"ns: \", ns)\n",
    "    s = []\n",
    "    # for cycle will split the original data into m splits\n",
    "    for i in range(m):\n",
    "    \ts.append([x[(ns*i):(ns*i+ns)],\n",
    "                  y[(ns*i):(ns*i+ns)]])\n",
    "\n",
    "    # print(\"s[0][0].shape: \", s[0][0].shape)\n",
    "\n",
    "    # deconstruct the array s into x_test and y_test\n",
    "    # remember that k is the parameter used to identify\n",
    "    # the index of the split that will be used as test\n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(m):\n",
    "        if (i==k):\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "\n",
    "    # x_train and y_train are arrays of arrays where\n",
    "    # each element is a split of the original data\n",
    "    # each element is an numpy array\n",
    "    # print(\"x_train[0].shape: \", x_train[0].shape)\n",
    "    # print(\"y_train[0].shape: \", y_train[0].shape)\n",
    "\n",
    "    # reshape the arrays to have desired shape, each one will\n",
    "    # merge all the splits inside a single array\n",
    "    #first we create numpy array\n",
    "    npyx = np.array(x_train)\n",
    "    npyy = np.array(y_train)\n",
    "\n",
    "    # print the shape\n",
    "    # print(\"npyx.shape: \", npyx.shape)\n",
    "    # print(\"npyy.shape: \", npyy.shape)\n",
    "\n",
    "    # reshape the array to have the desired shape\n",
    "    x_train = np.array(x_train).reshape(((m-1)*ns,30))\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "\n",
    "    # print(\"x_train.shape: \", x_train.shape)\n",
    "    # print(\"y_train.shape: \", y_train.shape)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected code defines a function called `split` that takes four arguments: `x`, `y`, `k`, and `m`. `x` is a NumPy array representing the feature matrix of a dataset, `y` is a NumPy array representing the target vector of the dataset, `k` is an integer representing the index of the test set, and `m` is an integer representing the number of folds in the cross-validation.\n",
    "\n",
    "The `split` function performs k-fold cross-validation by splitting the dataset into `m` equally sized folds, and using one fold as the test set and the remaining folds as the training set. Specifically, the function splits the dataset into `m` folds, where each fold contains a subset of the samples in `x` and `y`. The function then selects the `k`-th fold as the test set, and combines the remaining folds into a single training set.\n",
    "\n",
    "The function returns four NumPy arrays: `x_train`, `y_train`, `x_test`, and `y_test`. `x_train` and `y_train` represent the feature matrix and target vector of the training set, respectively, and are created by combining all of the folds except for the `k`-th fold. `x_test` and `y_test` represent the feature matrix and target vector of the test set, respectively, and are created by selecting the `k`-th fold.\n",
    "\n",
    "Overall, the `split` function is a helper function that performs k-fold cross-validation by splitting the dataset into `m` folds and selecting one fold as the test set. It returns the training and test sets as NumPy arrays that can be used for training and evaluating a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pp(z,k,s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" % (s, z[k].mean(), z[k].std()/np.sqrt(m)), end='')\n",
    "    for i in range(m):\n",
    "        print(\"%0.4f \" % z[k,i], end='')\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected code defines a function called `pp` that takes three arguments: `z`, `k`, and `s`. `z` is a NumPy array representing the results of a cross-validation experiment, `k` is an integer representing the index of the test fold, and `s` is a string representing the name of the experiment.\n",
    "\n",
    "The `pp` function prints a summary of the cross-validation results for the test fold specified by `k`. Specifically, the function prints the mean and standard error of the test scores for the test fold, as well as the test scores for each of the training folds. The mean and standard error are printed in a formatted string that includes the name of the experiment, and the test scores for each training fold are printed as a space-separated list of floating-point numbers.\n",
    "\n",
    "The function does not return any values, but instead prints the summary to the console using the `print()` function.\n",
    "\n",
    "Overall, the `pp` function is a helper function that prints a summary of the cross-validation results for a single test fold. It is typically used in conjunction with a cross-validation function to evaluate the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest            : 0.9327 +/- 0.0032 | 0.9204 0.9381 0.9292 0.9381 0.9381 \n",
      "3-NN               : 0.9646 +/- 0.0079 | 0.9381 0.9646 0.9735 0.9558 0.9912 \n",
      "7-NN               : 0.9681 +/- 0.0081 | 0.9558 0.9469 0.9646 0.9735 1.0000 \n",
      "Naive Bayes        : 0.9363 +/- 0.0039 | 0.9381 0.9381 0.9204 0.9381 0.9469 \n",
      "Decision Tree      : 0.9239 +/- 0.0040 | 0.9204 0.9115 0.9204 0.9381 0.9292 \n",
      "Random Forest (5)  : 0.9487 +/- 0.0058 | 0.9381 0.9469 0.9381 0.9469 0.9735 \n",
      "Random Forest (50) : 0.9558 +/- 0.0079 | 0.9646 0.9469 0.9292 0.9558 0.9823 \n",
      "SVM (linear)       : 0.9664 +/- 0.0068 | 0.9381 0.9735 0.9823 0.9646 0.9735 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = np.load(\"../data/breast/bc_features_standard.npy\")\n",
    "y = np.load(\"../data/breast/bc_labels.npy\")\n",
    "idx = np.argsort(np.random.random(y.shape[0]))\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "m = 5 #int(sys.argv[1])\n",
    "z = np.zeros((8,m))\n",
    "\n",
    "for k in range(m):\n",
    "    x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "    z[0,k] = run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    z[1,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    z[2,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    z[3,k] = run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    z[4,k] = run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    z[5,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    z[6,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    z[7,k] = run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "pp(z,0,\"Nearest\"); \n",
    "pp(z,1,\"3-NN\")\n",
    "pp(z,2,\"7-NN\");    \n",
    "pp(z,3,\"Naive Bayes\")\n",
    "pp(z,4,\"Decision Tree\");    \n",
    "pp(z,5,\"Random Forest (5)\")\n",
    "pp(z,6,\"Random Forest (50)\");    \n",
    "pp(z,7,\"SVM (linear)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp2(z,s):\n",
    "    m = z.shape[0]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f \" % (s, z.mean(), z.std()/np.sqrt(m)), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92920354 0.92566372 0.93097345 0.92920354 0.92920354 0.92743363\n",
      " 0.93097345 0.92920354 0.93097345 0.93274336 0.92920354 0.92920354\n",
      " 0.93097345 0.92920354 0.93097345 0.93097345 0.92920354 0.92743363\n",
      " 0.93097345 0.93274336 0.93097345 0.93097345 0.92920354 0.92743363\n",
      " 0.93097345 0.92743363 0.93097345 0.92743363 0.92920354 0.93097345\n",
      " 0.93097345 0.92743363 0.92920354 0.93097345 0.92743363 0.92920354\n",
      " 0.93274336 0.92743363 0.93097345 0.93097345 0.93097345 0.92743363\n",
      " 0.93274336 0.92920354 0.92743363 0.93097345 0.93097345 0.92920354\n",
      " 0.93274336 0.93097345]\n",
      "Nearest            : 0.9298 +/- 0.0002 \n",
      "3-NN               : 0.9658 +/- 0.0005 \n",
      "7-NN               : 0.9653 +/- 0.0005 \n",
      "Naive Bayes        : 0.9328 +/- 0.0005 \n",
      "Decision Tree      : 0.9250 +/- 0.0012 \n",
      "Random Forest (5)  : 0.9471 +/- 0.0009 \n",
      "Random Forest (50) : 0.9589 +/- 0.0006 \n",
      "SVM (linear)       : 0.9725 +/- 0.0005 \n",
      "Final Results\n",
      "Nearest Centroid:  0.9298407079646017\n",
      "3-NN:  0.9658053097345133\n",
      "7-NN:  0.9653097345132744\n",
      "Naive Bayes:  0.9328141592920354\n",
      "Decision Tree:  0.9249911504424779\n",
      "Random Forest (5):  0.9471150442477877\n",
      "Random Forest (50):  0.9589026548672566\n",
      "SVM (linear):  0.9724601769911504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration = 50\n",
    "x = np.load(\"../data/breast/bc_features_standard.npy\")\n",
    "y = np.load(\"../data/breast/bc_labels.npy\")\n",
    "\n",
    "z = np.zeros((8,m))\n",
    "final = np.zeros((8,iteration))\n",
    "# loop for the number of iterations\n",
    "m = 5 #int(sys.argv[1])\n",
    "for i in range(iteration):\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        z[0,k] = run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "        z[1,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "        z[2,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "        z[3,k] = run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "        z[4,k] = run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "        z[5,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "        z[6,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "        z[7,k] = run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    \n",
    "    final[0,i] = z[0].mean()\n",
    "    final[1,i] = z[1].mean()\n",
    "    final[2,i] = z[2].mean()\n",
    "    final[3,i] = z[3].mean()\n",
    "    final[4,i] = z[4].mean()\n",
    "    final[5,i] = z[5].mean()\n",
    "    final[6,i] = z[6].mean()\n",
    "    final[7,i] = z[7].mean()\n",
    "\n",
    "print(final[0])\n",
    "\n",
    "pp2(final[0],\"Nearest\");\n",
    "pp2(final[1],\"3-NN\")\n",
    "pp2(final[2],\"7-NN\");\n",
    "pp2(final[3],\"Naive Bayes\")\n",
    "pp2(final[4],\"Decision Tree\");\n",
    "pp2(final[5],\"Random Forest (5)\")\n",
    "pp2(final[6],\"Random Forest (50)\");\n",
    "pp2(final[7],\"SVM (linear)\")\n",
    "\n",
    "print(\"Final Results\")\n",
    "print(\"Nearest Centroid: \", final[0].mean())\n",
    "print(\"3-NN: \", final[1].mean())\n",
    "print(\"7-NN: \", final[2].mean())\n",
    "print(\"Naive Bayes: \", final[3].mean())\n",
    "print(\"Decision Tree: \", final[4].mean())\n",
    "print(\"Random Forest (5): \", final[5].mean())\n",
    "print(\"Random Forest (50): \", final[6].mean())\n",
    "print(\"SVM (linear): \", final[7].mean())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
